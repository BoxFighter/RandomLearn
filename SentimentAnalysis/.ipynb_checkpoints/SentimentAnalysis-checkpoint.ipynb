{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 电影评论情感分类     \n",
    "\n",
    "本实验将中文电影评论分类为“positive”、“negative”。\n",
    "\n",
    "首先导入必要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "\n",
    "数据集包括：\n",
    "1. 训练集。包含2W条左右中文电影评论，其中正负向评论各1W条左右。\n",
    "2. 验证集。包含6K条左右中文电影评论，其中正负向评论各3K条左右。\n",
    "3. 测试集。包含360条左右中文电影评论，其中正负向评论各180条左右。\n",
    "4. 预训练词向量。中文维基百科词向量word2vec。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 构建词汇表并存储，形如{word: id}："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2id(file, save_to_path=None):\n",
    "    \"\"\"\n",
    "    :param file: word2id保存地址\n",
    "    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    word2id = {'_PAD_': 0}\n",
    "    path = ['./Dataset/train.txt', './Dataset/validation.txt']\n",
    "    \n",
    "    for _path in path:\n",
    "        with open(_path, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                sp = line.strip().split()\n",
    "                for word in sp[1:]:\n",
    "                    if word not in word2id.keys():\n",
    "                        word2id[word] = len(word2id)\n",
    "    if save_to_path:                    \n",
    "        with open(file, 'w', encoding='utf-8') as f:\n",
    "            for w in word2id:\n",
    "                f.write(w+'\\t')\n",
    "                f.write(str(word2id[w]))\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return word2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)基于预训练的word2vec构建训练语料中所含词语的word2vec："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word2vec(fname, word2id, save_to_path=None):\n",
    "    \"\"\"\n",
    "    :param fname: 预训练的word2vec.\n",
    "    :param word2id: 语料文本中包含的词汇集.\n",
    "    :param save_to_path: 保存训练语料库中的词组对应的word2vec到本地\n",
    "    :return: 语料文本中词汇集对应的word2vec向量{id: word2vec}.\n",
    "    \"\"\"\n",
    "    n_words = max(word2id.values()) + 1\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    word_vecs = np.array(np.random.uniform(-1., 1., [n_words, model.vector_size]))\n",
    "    for word in word2id.keys():\n",
    "        try:\n",
    "            word_vecs[word2id[word]] = model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if save_to_path:\n",
    "        with open(save_to_path, 'w', encoding='utf-8') as f:\n",
    "            for vec in word_vecs:\n",
    "                vec = [str(w) for w in vec]\n",
    "                f.write(' '.join(vec))\n",
    "                f.write('\\n')\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)分类类别以及id对应词典{pos:0, neg:1}："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_id(classes=None):\n",
    "    \"\"\"\n",
    "    :param classes: 分类标签；默认为0:pos, 1:neg\n",
    "    :return: {分类标签：id}\n",
    "    \"\"\"\n",
    "    if not classes:\n",
    "        classes = ['0', '1']\n",
    "    cat2id = {cat: idx for (idx, cat) in enumerate(classes)}\n",
    "    return classes, cat2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4)加载语料库：train/dev/test："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path, word2id, max_sen_len=50):\n",
    "    \"\"\"\n",
    "    :param path: 样本语料库的文件\n",
    "    :return: 文本内容contents，以及分类标签labels(onehot形式)\n",
    "    \"\"\"\n",
    "    _, cat2id = cat_to_id()\n",
    "    contents, labels = [], []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            sp = line.strip().split()\n",
    "            label = sp[0]\n",
    "            content = [word2id.get(w, 0) for w in sp[1:]]\n",
    "            content = content[:max_sen_len]\n",
    "            if len(content) < max_sen_len:\n",
    "                content += [word2id['_PAD_']] * (max_sen_len - len(content))\n",
    "            labels.append(label)\n",
    "            contents.append(content)\n",
    "    counter = Counter(labels)\n",
    "    print('总样本数为：%d' % (len(labels)))\n",
    "    print('各个类别样本数如下：')\n",
    "    for w in counter:\n",
    "        print(w, counter[w])\n",
    "\n",
    "    contents = np.asarray(contents)\n",
    "    labels = np.array([cat2id[l] for l in labels])\n",
    "\n",
    "    return contents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train corpus load: \n",
      "总样本数为：19998\n",
      "各个类别样本数如下：\n",
      "1 9999\n",
      "0 9999\n",
      "\n",
      "validation corpus load: \n",
      "总样本数为：5629\n",
      "各个类别样本数如下：\n",
      "1 2812\n",
      "0 2817\n",
      "\n",
      "test corpus load: \n",
      "总样本数为：369\n",
      "各个类别样本数如下：\n",
      "1 187\n",
      "0 182\n"
     ]
    }
   ],
   "source": [
    "word2id = build_word2id('./Dataset/word2id.txt')\n",
    "# print(word2id)\n",
    "word2vec = build_word2vec('./Dataset/wiki_word2vec_50.bin', word2id)\n",
    "assert word2vec.shape == (58954, 50)\n",
    "# print(word2vec)\n",
    "print('train corpus load: ')\n",
    "train_contents, train_labels = load_corpus('./Dataset/train.txt', word2id, max_sen_len=50)\n",
    "print('\\nvalidation corpus load: ')\n",
    "val_contents, val_labels = load_corpus('./Dataset/validation.txt', word2id, max_sen_len=50)\n",
    "print('\\ntest corpus load: ')\n",
    "test_contents, test_labels = load_corpus('./Dataset/test.txt', word2id, max_sen_len=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过数据预处理，数据的格式如下：\n",
    "- x: [1434, 5454, 2323, ..., 0, 0, 0]\n",
    "- y: [1]\n",
    "\n",
    "x为构成一条语句的单词所对应的id。 y为类别: pos：0, neg：1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型\n",
    "\n",
    "构建Text-CNN模型，模型结构如下图所示：\n",
    "\n",
    "![](https://pic.downk.cc/item/5e9bc837c2a9a83be5a96d95.jpg)\n",
    "\n",
    "模型包括词嵌入层、卷积层、池化层和全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)配置模型相关参数，在COINFIG类中完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG():\n",
    "    update_w2v = True           # 是否在训练中更新w2v\n",
    "    vocab_size = 58954          # 词汇量，与word2id中的词汇量一致\n",
    "    n_class = 2                 # 分类数：分别为pos和neg\n",
    "    embedding_dim = 50          # 词向量维度\n",
    "    drop_keep_prob = 0.5        # dropout层，参数keep的比例\n",
    "    kernel_num = 64            # 卷积层filter的数量\n",
    "    kernel_size = [3,4,5]       # 卷积核的尺寸\n",
    "    pretrained_embed = word2vec # 预训练的词嵌入模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)构建Text-CNN模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TextCNN, self).__init__()\n",
    "        update_w2v = config.update_w2v\n",
    "        vocab_size = config.vocab_size\n",
    "        n_class = config.n_class\n",
    "        embedding_dim = config.embedding_dim\n",
    "        kernel_num = config.kernel_num\n",
    "        kernel_size = config.kernel_size\n",
    "        drop_keep_prob = config.drop_keep_prob\n",
    "        pretrained_embed = config.pretrained_embed\n",
    "        \n",
    "        # 使用预训练的词向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embed))\n",
    "        self.embedding.weight.requires_grad = update_w2v\n",
    "        # 卷积层\n",
    "        self.conv1 = nn.Conv2d(1, kernel_num, (kernel_size[0], embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(1, kernel_num, (kernel_size[1], embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(1, kernel_num, (kernel_size[2], embedding_dim))\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(drop_keep_prob)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(len(kernel_size) * kernel_num, n_class)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_and_pool(x, conv):\n",
    "        # x: (batch, 1, sentence_length,  )\n",
    "        x = conv(x)\n",
    "        # x: (batch, kernel_num, H_out, 1)\n",
    "        x = F.relu(x.squeeze(3))\n",
    "        # x: (batch, kernel_num, H_out)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        #  (batch, kernel_num)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.int64)\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = self.conv_and_pool(x, self.conv1)  # (batch, kernel_num)\n",
    "        x2 = self.conv_and_pool(x, self.conv2)  # (batch, kernel_num)\n",
    "        x3 = self.conv_and_pool(x, self.conv3)  # (batch, kernel_num)\n",
    "        x = torch.cat((x1, x2, x3), 1)  # (batch, 3 * kernel_num)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)设置超参数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CONFIG()          # 配置模型参数\n",
    "learning_rate = 0.001      # 学习率     \n",
    "BATCH_SIZE = 50            # 训练批量\n",
    "EPOCHS = 10                 # 训练轮数\n",
    "model_path = None          # 预训练模型路径\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)加载训练数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练用的数据\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_contents).type(torch.float), \n",
    "                              torch.from_numpy(train_labels).type(torch.long))\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n",
    "                              shuffle = True, num_workers = 2)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(val_contents).type(torch.float), \n",
    "                              torch.from_numpy(val_labels).type(torch.long))\n",
    "val_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n",
    "                              shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型，是否继续上一次的训练\n",
    "model = TextCNN(config)\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "    \n",
    "# 设置优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 设置损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,epoch):\n",
    "    # 定义训练过程\n",
    "    train_loss,train_acc = 0.0,0.0\n",
    "    count, correct = 0,0\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        correct += (output.argmax(1) == batch_y).float().sum().item()\n",
    "        count += len(batch_x)\n",
    "            \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print('train epoch: {} [{}/{} ({:.0f}%)]\\tloss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(batch_x), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "                \n",
    "    train_loss *= BATCH_SIZE\n",
    "    train_loss /= len(dataloader.dataset)\n",
    "    train_acc = correct/count\n",
    "    print('\\ntrain epoch: {}\\taverage loss: {:.6f}\\taccuracy:{:.4f}%\\n'.format(epoch+1,train_loss,100.*train_acc))\n",
    "    \n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader,epoch):\n",
    "    # 验证过程\n",
    "    val_loss,val_acc = 0.0,0.0\n",
    "    count, correct = 0,0\n",
    "    for _, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        val_loss += loss.item()\n",
    "        correct += (output.argmax(1) == batch_y).float().sum().item()\n",
    "        count += len(batch_x)\n",
    "    \n",
    "    val_loss *= BATCH_SIZE\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    val_acc = correct/count\n",
    "    # 打印准确率\n",
    "    print('validation:train epoch: {}\\taverage loss: {:.6f}\\t accuracy:{:.2f}%\\n'.format(epoch,val_loss,100*val_acc))\n",
    "    \n",
    "    return val_loss,val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [4950/19998 (25%)]\tloss: 0.629728\n",
      "train epoch: 1 [9950/19998 (50%)]\tloss: 0.507787\n",
      "train epoch: 1 [14950/19998 (75%)]\tloss: 0.473537\n",
      "train epoch: 1 [19152/19998 (100%)]\tloss: 0.467112\n",
      "\n",
      "train epoch: 1\taverage loss: 0.535830\taccuracy:71.9872%\n",
      "\n",
      "validation:train epoch: 0\taverage loss: 0.386854\t accuracy:83.26%.\n",
      "train epoch: 2 [4950/19998 (25%)]\tloss: 0.317643\n",
      "train epoch: 2 [9950/19998 (50%)]\tloss: 0.370819\n",
      "train epoch: 2 [14950/19998 (75%)]\tloss: 0.492338\n",
      "train epoch: 2 [19152/19998 (100%)]\tloss: 0.187140\n",
      "\n",
      "train epoch: 2\taverage loss: 0.363383\taccuracy:83.9584%\n",
      "\n",
      "validation:train epoch: 1\taverage loss: 0.263679\t accuracy:89.85%.\n",
      "train epoch: 3 [4950/19998 (25%)]\tloss: 0.265955\n",
      "train epoch: 3 [9950/19998 (50%)]\tloss: 0.166393\n",
      "train epoch: 3 [14950/19998 (75%)]\tloss: 0.274880\n",
      "train epoch: 3 [19152/19998 (100%)]\tloss: 0.232105\n",
      "\n",
      "train epoch: 3\taverage loss: 0.256492\taccuracy:89.8190%\n",
      "\n",
      "validation:train epoch: 2\taverage loss: 0.169681\t accuracy:94.21%.\n",
      "train epoch: 4 [4950/19998 (25%)]\tloss: 0.085649\n",
      "train epoch: 4 [9950/19998 (50%)]\tloss: 0.098642\n",
      "train epoch: 4 [14950/19998 (75%)]\tloss: 0.131401\n",
      "train epoch: 4 [19152/19998 (100%)]\tloss: 0.264086\n",
      "\n",
      "train epoch: 4\taverage loss: 0.169045\taccuracy:93.7694%\n",
      "\n",
      "validation:train epoch: 3\taverage loss: 0.103601\t accuracy:96.71%.\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_acces = []\n",
    "val_losses = []\n",
    "val_acces = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss,tr_acc = train(train_dataloader,epoch)\n",
    "    val_loss,val_acc = validation(val_dataloader,epoch)\n",
    "    train_losses.append(tr_loss)\n",
    "    train_acces.append(tr_acc)\n",
    "    \n",
    "# 保存模型\n",
    "model_pth = 'model_' + str(time.time()) + '.pth'\n",
    "torch.save(model.state_dict(), model_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型\n",
    "\n",
    "测试模型在测试集的准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "model_path = model_pth    # 模型路径\n",
    "# 加载测试集\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_contents).type(torch.float), \n",
    "                            torch.from_numpy(test_labels).type(torch.long))\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, \n",
    "                            shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "\n",
    "    # 读取模型\n",
    "    model = TextCNN(config)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # 测试过程\n",
    "    count, correct = 0, 0\n",
    "    for _, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "        output = model(batch_x)\n",
    "        correct += (output.argmax(1) == batch_y).float().sum().item()\n",
    "        count += len(batch_x)\n",
    "    \n",
    "    # 打印准确率\n",
    "    print('test accuracy:{:.2f}%.'.format(100*correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:84.01%.\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
