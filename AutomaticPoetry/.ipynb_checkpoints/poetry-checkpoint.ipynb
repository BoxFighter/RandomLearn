{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入预处理的数据\n",
    "datas = np.load(\"./tang.npz\")\n",
    "data = datas['data']\n",
    "ix2word = datas['ix2word'].item()\n",
    "word2ix = datas['word2ix'].item()\n",
    "    \n",
    "# 转为torch.Tensor\n",
    "data = torch.from_numpy(data)\n",
    "train_loader = DataLoader(data, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2)\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "\n",
    "        embeds = self.embedding(input)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        output = self.linear(output.view(seq_len * batch_size, -1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型，是否继续上一次的训练\n",
    "model = PoetryModel(len(word2ix),embedding_dim = 128,hidden_dim = 256)\n",
    "\n",
    "model_path = None          # 预训练模型路径\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, ix2word, word2ix, device, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = data.long().transpose(1, 0).contiguous()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input, target = data[:-1, :], data[1:, :]\n",
    "        output, _ = model(input)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "        if (batch_idx+1) % 200 == 0:\n",
    "            print('train epoch: {} [{}/{} ({:.0f}%)]\\tloss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data[1]), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "    train_loss *= BATCH_SIZE\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('\\ntrain epoch: {}\\t average loss: {:.6f}\\n'.format(epoch,train_loss))\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [3184/57580 (6%)]\tloss: 2.469445\n",
      "train epoch: 1 [6384/57580 (11%)]\tloss: 3.656164\n",
      "train epoch: 1 [9584/57580 (17%)]\tloss: 2.830234\n",
      "train epoch: 1 [12784/57580 (22%)]\tloss: 3.004339\n",
      "train epoch: 1 [15984/57580 (28%)]\tloss: 2.533713\n",
      "train epoch: 1 [19184/57580 (33%)]\tloss: 2.563027\n",
      "train epoch: 1 [22384/57580 (39%)]\tloss: 2.505629\n",
      "train epoch: 1 [25584/57580 (44%)]\tloss: 2.112583\n",
      "train epoch: 1 [28784/57580 (50%)]\tloss: 1.920828\n",
      "train epoch: 1 [31984/57580 (56%)]\tloss: 2.196493\n",
      "train epoch: 1 [35184/57580 (61%)]\tloss: 2.263508\n",
      "train epoch: 1 [38384/57580 (67%)]\tloss: 2.162863\n",
      "train epoch: 1 [41584/57580 (72%)]\tloss: 2.266578\n",
      "train epoch: 1 [44784/57580 (78%)]\tloss: 2.251940\n",
      "train epoch: 1 [47984/57580 (83%)]\tloss: 2.211154\n",
      "train epoch: 1 [51184/57580 (89%)]\tloss: 2.362715\n",
      "train epoch: 1 [54384/57580 (94%)]\tloss: 2.823956\n",
      "\n",
      "train epoch: 1\t average loss: 2.453792\n",
      "\n",
      "train epoch: 2 [3184/57580 (6%)]\tloss: 1.997899\n",
      "train epoch: 2 [6384/57580 (11%)]\tloss: 2.677930\n",
      "train epoch: 2 [9584/57580 (17%)]\tloss: 2.444789\n",
      "train epoch: 2 [12784/57580 (22%)]\tloss: 1.885349\n",
      "train epoch: 2 [15984/57580 (28%)]\tloss: 2.092296\n",
      "train epoch: 2 [19184/57580 (33%)]\tloss: 2.321712\n",
      "train epoch: 2 [22384/57580 (39%)]\tloss: 2.427544\n",
      "train epoch: 2 [25584/57580 (44%)]\tloss: 2.567299\n",
      "train epoch: 2 [28784/57580 (50%)]\tloss: 2.562108\n",
      "train epoch: 2 [31984/57580 (56%)]\tloss: 2.570652\n",
      "train epoch: 2 [35184/57580 (61%)]\tloss: 2.080362\n",
      "train epoch: 2 [38384/57580 (67%)]\tloss: 1.940165\n",
      "train epoch: 2 [41584/57580 (72%)]\tloss: 2.329484\n",
      "train epoch: 2 [44784/57580 (78%)]\tloss: 1.887087\n",
      "train epoch: 2 [47984/57580 (83%)]\tloss: 1.966232\n",
      "train epoch: 2 [51184/57580 (89%)]\tloss: 2.063430\n",
      "train epoch: 2 [54384/57580 (94%)]\tloss: 1.922111\n",
      "\n",
      "train epoch: 2\t average loss: 2.180018\n",
      "\n",
      "train epoch: 3 [3184/57580 (6%)]\tloss: 2.199260\n",
      "train epoch: 3 [6384/57580 (11%)]\tloss: 2.268003\n",
      "train epoch: 3 [9584/57580 (17%)]\tloss: 2.144368\n",
      "train epoch: 3 [12784/57580 (22%)]\tloss: 1.957354\n",
      "train epoch: 3 [15984/57580 (28%)]\tloss: 1.977028\n",
      "train epoch: 3 [19184/57580 (33%)]\tloss: 2.086961\n",
      "train epoch: 3 [22384/57580 (39%)]\tloss: 2.790386\n",
      "train epoch: 3 [25584/57580 (44%)]\tloss: 2.137955\n",
      "train epoch: 3 [28784/57580 (50%)]\tloss: 1.666964\n",
      "train epoch: 3 [31984/57580 (56%)]\tloss: 1.924477\n",
      "train epoch: 3 [35184/57580 (61%)]\tloss: 1.925080\n",
      "train epoch: 3 [38384/57580 (67%)]\tloss: 1.962848\n",
      "train epoch: 3 [41584/57580 (72%)]\tloss: 1.926018\n",
      "train epoch: 3 [44784/57580 (78%)]\tloss: 1.662280\n",
      "train epoch: 3 [47984/57580 (83%)]\tloss: 2.324435\n",
      "train epoch: 3 [51184/57580 (89%)]\tloss: 2.539874\n",
      "train epoch: 3 [54384/57580 (94%)]\tloss: 2.145408\n",
      "\n",
      "train epoch: 3\t average loss: 2.094503\n",
      "\n",
      "train epoch: 4 [3184/57580 (6%)]\tloss: 1.805206\n",
      "train epoch: 4 [6384/57580 (11%)]\tloss: 1.713874\n",
      "train epoch: 4 [9584/57580 (17%)]\tloss: 2.062760\n",
      "train epoch: 4 [12784/57580 (22%)]\tloss: 2.370543\n",
      "train epoch: 4 [15984/57580 (28%)]\tloss: 2.006826\n",
      "train epoch: 4 [19184/57580 (33%)]\tloss: 2.334563\n",
      "train epoch: 4 [22384/57580 (39%)]\tloss: 2.014790\n",
      "train epoch: 4 [25584/57580 (44%)]\tloss: 1.845157\n",
      "train epoch: 4 [28784/57580 (50%)]\tloss: 2.395512\n",
      "train epoch: 4 [31984/57580 (56%)]\tloss: 1.649153\n",
      "train epoch: 4 [35184/57580 (61%)]\tloss: 2.218032\n",
      "train epoch: 4 [38384/57580 (67%)]\tloss: 2.436201\n",
      "train epoch: 4 [41584/57580 (72%)]\tloss: 1.982601\n",
      "train epoch: 4 [44784/57580 (78%)]\tloss: 1.987837\n",
      "train epoch: 4 [47984/57580 (83%)]\tloss: 1.713771\n",
      "train epoch: 4 [51184/57580 (89%)]\tloss: 1.694652\n",
      "train epoch: 4 [54384/57580 (94%)]\tloss: 2.030915\n",
      "\n",
      "train epoch: 4\t average loss: 2.040116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    tr_loss = train(model,train_loader,ix2word,word2ix,DEVICE,optimizer,scheduler,epoch)\n",
    "    train_losses.append(tr_loss)\n",
    "    \n",
    "# 保存模型\n",
    "filename = \"model\" + str(time.time()) + \".pth\"\n",
    "torch.save(model.state_dict(), filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoetryModel(\n",
       "  (embedding): Embedding(8293, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2)\n",
       "  (linear): Linear(in_features=256, out_features=8293, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取模型\n",
    "model_path = filename      # 模型路径\n",
    "model = PoetryModel(len(word2ix),embedding_dim = 128,hidden_dim = 256)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words=None):\n",
    "    # 读取唐诗的第一句\n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 设置第一个词为<START>\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    input = input.to(DEVICE)\n",
    "    hidden = None\n",
    "    \n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input, hidden)\n",
    "            input = input.data.new([word2ix[word]]).view(1, 1)\n",
    "\n",
    "    # 生成唐诗\n",
    "    for i in range(max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        # 读取第一句\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
    "        # 生成后面的句子\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            results.append(w)\n",
    "            input = input.data.new([top_index]).view(1, 1)\n",
    "        # 结束标志\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "白日依山尽，江南见旧游。\n",
      "江湖春草绿，山色白云生。\n",
      "野渡青山远，江深月色微。\n",
      "云山寒水阔，山色白云生。\n",
      "野渡青山远，山深竹树新。\n",
      "野花开白发，山鸟有余情。\n",
      "白发今如此，青山不可寻。\n",
      "一声闻不得，万里独悠悠。\n",
      "远客何时见，孤舟望远公。\n",
      "高楼望乡路，回首望乡情。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_words = '白日依山尽'  # 唐诗的第一句\n",
    "max_gen_len = 128         # 生成唐诗的最长长度\n",
    "\n",
    "prefix_words = None\n",
    "results = generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words)\n",
    "poetry = ''\n",
    "for i in results:\n",
    "    poetry += i\n",
    "    if i == '。':\n",
    "        poetry += '\\n'\n",
    "        \n",
    "print(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
